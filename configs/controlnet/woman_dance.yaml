# accelerate launch train_tuneavideo_controlnet.py --config="configs/controlnet/woman_dance.yaml"

pretrained_model_path: "../diffusers_controlnet/models/control_sd15_openpose"
output_dir: "./outputs/control_woman_dance"

train_data:
  video_path: "data/woman_dance/original.mp4"
  controlnet_hint_path: "data/woman_dance/openpose.mp4"
  prompt: "a woman is dancing"
  n_sample_frames: 8
  width: 512
  height: 512
  sample_start_idx: 0
  sample_frame_rate: 1

validation_data:
  prompts:
    - "a firefighter is dancing"
    - "a panda is dancing"
  video_length: 8
  width: 512
  height: 512
  num_inference_steps: 50
  guidance_scale: 7.5

learning_rate: 3e-5
train_batch_size: 1
max_train_steps: 300
checkpointing_steps: 1000
validation_steps: 100
trainable_modules:
  - "attn1.to_q"
  - "attn2.to_q"
  - "attn_temp"

seed: 0
mixed_precision: fp16
# mixed_precision: "no"
use_8bit_adam: False
gradient_checkpointing: True
enable_xformers_memory_efficient_attention: True
